{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f6b9eb",
   "metadata": {
    "papermill": {
     "duration": 0.005323,
     "end_time": "2024-11-27T17:21:54.832898",
     "exception": false,
     "start_time": "2024-11-27T17:21:54.827575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ArXiv Paper Analysis Using Gemini's Long Context Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01d08c",
   "metadata": {
    "papermill": {
     "duration": 0.004573,
     "end_time": "2024-11-27T17:21:54.842166",
     "exception": false,
     "start_time": "2024-11-27T17:21:54.837593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "This notebook demonstrates the use of Gemini 1.5's large context window to analyze a comprehensive dataset of ArXiv papers. We process and analyze abstracts from multiple AI-related categories (including Computer Vision, Machine Learning, Natural Language Processing and more) to identify research trends, methodological innovations, and future directions in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffe154",
   "metadata": {
    "papermill": {
     "duration": 0.00424,
     "end_time": "2024-11-27T17:21:54.850997",
     "exception": false,
     "start_time": "2024-11-27T17:21:54.846757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Abstract\n",
    "This study demonstrates the application of Gemini 1.5's large context window capabilities in analyzing comprehensive research trends from the ArXiv database, containing over 2.6 million papers. Unlike traditional approaches that rely on vector databases or retrieval-augmented generation (RAG), we leverage Gemini's ability to process extensive text directly while maintaining coherent understanding across content. Our methodology employs a novel content validation system and efficient context caching to analyze research papers across multiple domains, with a particular focus on computer science and machine learning categories from 2018 to 2024.\n",
    "Our analysis pipeline processes papers in optimized batches of 250,000 words, implementing a rolling context cache to maintain coherence across sections. A sophisticated validation system using content markers and overlap detection ensures the model's comprehension and accurate reference of specific content, achieving a 76.8% content coverage rate. The system successfully processed over 28,000 papers from selected categories, analyzing more than 1.6 million total words of content across 116 batches.\n",
    "Results reveal significant trends in research focus and methodological approaches. Machine learning (cs.LG) submissions showed the most dramatic growth, increasing from 10,498 papers in 2018 to 33,963 papers in early 2024. Computer Vision (cs.CV) and Artificial Intelligence (cs.AI) also demonstrated substantial growth, while Statistical Machine Learning (stat.ML) showed a decline in paper submissions after 2020, suggesting a shift in how researchers categorize their work.\n",
    "Our implementation achieved processing speeds exceeding 10,000 tokens per second while maintaining contextual understanding across the entire corpus. This study demonstrates that large context window models can effectively analyze extensive research databases without relying on traditional information retrieval methods, offering new possibilities for comprehensive research trend analysis and pattern recognition across academic literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5df7c0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:54.861587Z",
     "iopub.status.busy": "2024-11-27T17:21:54.861207Z",
     "iopub.status.idle": "2024-11-27T17:21:55.832082Z",
     "shell.execute_reply": "2024-11-27T17:21:55.830890Z"
    },
    "papermill": {
     "duration": 0.979048,
     "end_time": "2024-11-27T17:21:55.834467",
     "exception": false,
     "start_time": "2024-11-27T17:21:54.855419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/arxivjson/arxiv-metadata-oai-snapshot.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fdecd",
   "metadata": {
    "papermill": {
     "duration": 0.004268,
     "end_time": "2024-11-27T17:21:55.843467",
     "exception": false,
     "start_time": "2024-11-27T17:21:55.839199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Overview\n",
    "Dataset Overview\n",
    "We analyze ArXiv metadata (1.7M articles) focusing on:\n",
    "\n",
    "* Paper ID, authors, title, abstract\n",
    "* Categories/tags\n",
    "* Publication dates and version history\n",
    "\n",
    "Key focus: Computer science categories (cs.AI, cs.CV, cs.LG, cs.CL) and Statistical ML (stat.ML) from 2018-2024. Please refere to [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) for more information.\n",
    "\n",
    "#### Now, let's write some code. We start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11440fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:55.853962Z",
     "iopub.status.busy": "2024-11-27T17:21:55.853447Z",
     "iopub.status.idle": "2024-11-27T17:21:57.103187Z",
     "shell.execute_reply": "2024-11-27T17:21:57.102245Z"
    },
    "papermill": {
     "duration": 1.257517,
     "end_time": "2024-11-27T17:21:57.105457",
     "exception": false,
     "start_time": "2024-11-27T17:21:55.847940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, List, Iterator\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee735e",
   "metadata": {
    "papermill": {
     "duration": 0.004277,
     "end_time": "2024-11-27T17:21:57.114606",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.110329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Next, you need to initialize the API. To do so, create an API token on [Google AI Studio](https://ai.google.dev) and add it to the notebook as a [secret key](https://www.kaggle.com/discussions/product-feedback/114053). Please visit the referenced websites for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2538c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:57.126282Z",
     "iopub.status.busy": "2024-11-27T17:21:57.125665Z",
     "iopub.status.idle": "2024-11-27T17:21:57.446336Z",
     "shell.execute_reply": "2024-11-27T17:21:57.445430Z"
    },
    "papermill": {
     "duration": 0.328693,
     "end_time": "2024-11-27T17:21:57.448671",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.119978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize API\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"gen_api\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27172f59",
   "metadata": {
    "papermill": {
     "duration": 0.004225,
     "end_time": "2024-11-27T17:21:57.457769",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.453544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Here we config Gemini's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd59795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:57.468578Z",
     "iopub.status.busy": "2024-11-27T17:21:57.467790Z",
     "iopub.status.idle": "2024-11-27T17:21:57.474998Z",
     "shell.execute_reply": "2024-11-27T17:21:57.473950Z"
    },
    "papermill": {
     "duration": 0.014832,
     "end_time": "2024-11-27T17:21:57.477079",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.462247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GeminiConfig:\n",
    "    \"\"\"Configuration settings for Gemini API\"\"\"\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.8\n",
    "    top_k: int = 40\n",
    "    max_output_tokens: int = 2048\n",
    "    candidate_count: int = 1\n",
    "    batch_size: int = 50000\n",
    "    cache_size: int = 1000\n",
    "\n",
    "    def get_generation_config(self):\n",
    "        return genai.types.GenerationConfig(\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            top_k=self.top_k,\n",
    "            max_output_tokens=self.max_output_tokens,\n",
    "            candidate_count=self.candidate_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabf551",
   "metadata": {
    "papermill": {
     "duration": 0.004308,
     "end_time": "2024-11-27T17:21:57.486131",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.481823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Token Tracker Implementation\n",
    "\n",
    "TokenTracker monitors large text processing through Gemini's API:\n",
    "\n",
    "- Tracks words processed, estimated tokens (1.3x word count multiplier), batches\n",
    "- Monitors processing speed (tokens/second)\n",
    "- Tracks context window utilization against 2M token limit\n",
    "- Provides real-time progress updates\n",
    "\n",
    "The 1.3x multiplier for token estimation is approximate but sufficient for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18e8921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:57.496877Z",
     "iopub.status.busy": "2024-11-27T17:21:57.496475Z",
     "iopub.status.idle": "2024-11-27T17:21:57.506678Z",
     "shell.execute_reply": "2024-11-27T17:21:57.505699Z"
    },
    "papermill": {
     "duration": 0.017837,
     "end_time": "2024-11-27T17:21:57.508592",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.490755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenTracker:\n",
    "    \"\"\"Tracks token usage and context window statistics\"\"\"\n",
    "    def __init__(self, model_context_size: int = 2000000):\n",
    "        self.model_context_size = model_context_size\n",
    "        self.total_tokens = 0\n",
    "        self.total_words = 0\n",
    "        self.batches_processed = 0\n",
    "        self.start_time = None\n",
    "\n",
    "    def start_tracking(self, total_words: int):\n",
    "        \"\"\"Start tracking processing time\"\"\"\n",
    "        self.start_time = datetime.now()\n",
    "        print(f\"Starting processing of {total_words:,} words\")\n",
    "\n",
    "    def update(self, words: int):\n",
    "        \"\"\"Update tracking with new batch of words\"\"\"\n",
    "        self.total_words += words\n",
    "        estimated_tokens = int(words * 1.3)\n",
    "        self.total_tokens += estimated_tokens\n",
    "        self.batches_processed += 1\n",
    "        print(f\"Processed batch {self.batches_processed}: {words:,} words\")\n",
    "        \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current processing statistics\"\"\"\n",
    "        elapsed = datetime.now() - self.start_time if self.start_time else None\n",
    "        \n",
    "        return {\n",
    "            'total_words': self.total_words,\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'batches_processed': self.batches_processed,\n",
    "            'context_window_usage': (self.total_tokens / self.model_context_size) * 100,\n",
    "            'elapsed_seconds': elapsed.total_seconds() if elapsed else 0\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Print final stats\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        elapsed_minutes = stats['elapsed_seconds'] / 60\n",
    "        tokens_per_second = stats['total_tokens'] / stats['elapsed_seconds']\n",
    "        \n",
    "        print(\"\\nProcessing Summary:\")\n",
    "        print(f\"Total words processed: {stats['total_words']:,}\")\n",
    "        print(f\"Total tokens processed: {stats['total_tokens']:,}\")\n",
    "        print(f\"Batches processed: {stats['batches_processed']}\")\n",
    "        print(f\"Context window usage: {stats['context_window_usage']:.1f}%\")\n",
    "        print(f\"Processing time: {elapsed_minutes:.1f} minutes\")\n",
    "        print(f\"Processing speed: {tokens_per_second:.1f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e42483",
   "metadata": {
    "papermill": {
     "duration": 0.004402,
     "end_time": "2024-11-27T17:21:57.517731",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.513329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ArXiv Analyzer Implementation\n",
    "\n",
    "`ArxivAnalyzer` handles data processing through two core methods designed for efficient dataset exploration and sampling:\n",
    "\n",
    "### build_category_mapping\n",
    "This method performs initial dataset analysis:\n",
    "- Scans the full dataset (1.7M+ papers) to count papers in each ArXiv category\n",
    "- Generates a sorted mapping of category frequencies\n",
    "- Prints top 20 categories with paper counts\n",
    "- Returns complete category dictionary for later filtering\n",
    "\n",
    "Key utility:\n",
    "- Helps identify most active research areas\n",
    "- Guides category selection for detailed analysis\n",
    "- Provides dataset composition insights\n",
    "- Used internally to validate category filters\n",
    "\n",
    "### load_papers_by_category\n",
    "This method implements sophisticated paper selection:\n",
    "\n",
    "**Filtering Parameters:**\n",
    "- Categories: List of ArXiv categories (e.g., ['cs.AI', 'cs.LG'])\n",
    "- Date range: 2018-2024 default, customizable\n",
    "- Papers per year: 500 default, adjustable for memory/processing needs\n",
    "\n",
    "**Processing Steps:**\n",
    "1. Creates year-category matrix for paper collection\n",
    "2. Processes papers sequentially to minimize memory usage\n",
    "3. Filters by:\n",
    "   - Category membership\n",
    "   - Publication date\n",
    "   - Version information (keeps original submission date)\n",
    "4. Extracts key metadata:\n",
    "   - Title and abstract (primary analysis content)\n",
    "   - Authors and submission dates\n",
    "   - Category tags (for cross-category analysis)\n",
    "   - DOI and version history\n",
    "\n",
    "**Output:**\n",
    "- Returns pandas DataFrame with filtered papers\n",
    "- Prints summary statistics of paper distribution across years\n",
    "- Maintains temporal balance through per-year sampling\n",
    "\n",
    "**Memory Management:**\n",
    "- Streams papers instead of loading entire dataset\n",
    "- Implements sampling to keep memory usage reasonable\n",
    "- Enables processing of large paper collections on standard hardware\n",
    "\n",
    "The combination of these methods enables systematic analysis of research trends while managing computational resources effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d8a7d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:57.528683Z",
     "iopub.status.busy": "2024-11-27T17:21:57.528348Z",
     "iopub.status.idle": "2024-11-27T17:21:57.542489Z",
     "shell.execute_reply": "2024-11-27T17:21:57.541438Z"
    },
    "papermill": {
     "duration": 0.022331,
     "end_time": "2024-11-27T17:21:57.544565",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.522234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArxivAnalyzer:\n",
    "    \"\"\"Base class for analyzing ArXiv metadata\"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.categories = self.build_category_mapping()\n",
    "\n",
    "    def build_category_mapping(self):\n",
    "        \"\"\"Build category mapping from data\"\"\"\n",
    "        categories = {}\n",
    "        total_lines = sum(1 for _ in open(self.file_path))\n",
    "        print(f\"\\nBuilding category mapping from {total_lines:,} papers...\")\n",
    "        \n",
    "        with open(self.file_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 100000 == 0:\n",
    "                    print(f\"Processed {i:,} papers\")\n",
    "                paper = json.loads(line)\n",
    "                paper_cats = paper['categories'].split()\n",
    "                for category in paper_cats:\n",
    "                    categories[category] = categories.get(category, 0) + 1\n",
    "        top_n = 10\n",
    "        top_categories = dict(sorted(categories.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "        print(f\"\\nTop {top_n} categories by paper count:\")\n",
    "        for cat, count in top_categories.items():\n",
    "            print(f\"{cat}: {count:,} papers\")\n",
    "        \n",
    "        return dict(sorted(categories.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    def load_papers_by_category(self, categories: List[str], start_year=2018, \n",
    "                              end_year=2024, papers_per_year=500) -> pd.DataFrame:\n",
    "        \"\"\"Load papers for specified categories within date range\"\"\"\n",
    "        papers_by_year = {year: {cat: [] for cat in categories} \n",
    "                         for year in range(start_year, end_year + 1)}\n",
    "        \n",
    "        total_lines = sum(1 for _ in open(self.file_path))\n",
    "        print(f\"\\nLoading papers from {total_lines:,} entries...\")\n",
    "        \n",
    "        with open(self.file_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 100000 == 0:\n",
    "                    print(f\"Processed {i:,} papers\")\n",
    "                paper = json.loads(line)\n",
    "                paper_cats = paper['categories'].split()\n",
    "                date = datetime.strptime(paper['versions'][0]['created'], \n",
    "                                      '%a, %d %b %Y %H:%M:%S GMT')\n",
    "                \n",
    "                if start_year <= date.year <= end_year:\n",
    "                    for category in categories:\n",
    "                        if category in paper_cats:\n",
    "                            papers_by_year[date.year][category].append({\n",
    "                                'title': paper['title'],\n",
    "                                'abstract': paper['abstract'],\n",
    "                                'year': date.year,\n",
    "                                'categories': paper['categories'],\n",
    "                                'authors': paper['authors'],\n",
    "                                'doi': paper.get('doi', ''),\n",
    "                                'version_count': len(paper['versions'])\n",
    "                            })\n",
    "\n",
    "        sampled_papers = []\n",
    "        for year_cats in papers_by_year.values():\n",
    "            for papers in year_cats.values():\n",
    "                sampled_papers.extend(papers[:papers_per_year])\n",
    "        \n",
    "        df = pd.DataFrame(sampled_papers)\n",
    "        print(\"\\nPaper counts by year:\")\n",
    "        print(df['year'].value_counts().sort_index())\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26be587",
   "metadata": {
    "papermill": {
     "duration": 0.004267,
     "end_time": "2024-11-27T17:21:57.553473",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.549206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Research Analyzer Implementation\n",
    "`ResearchAnalyzer` processes content using Gemini 1.5 with three key components:\n",
    "\n",
    "**1. Content Processing**\n",
    "- Streams text in configurable batches (default 250K words)\n",
    "- Maintains semantic coherence (no mid-sentence breaks)\n",
    "- Uses rolling cache of previous analyses for context\n",
    "\n",
    "**2. Validation System**\n",
    "- Places markers every quarter-batch\n",
    "- Creates ±10 word context windows around markers\n",
    "- Tracks content references in analysis\n",
    "- Requires 30% word overlap for valid references\n",
    "\n",
    "**3. Analysis Process**\n",
    "- Processes content in batches\n",
    "- Validates coverage and cross-references\n",
    "- Generates comprehensive summary for multi-batch analysis\n",
    "- Reports processing statistics and content coverage\n",
    "\n",
    "The validation ensures the model processes content thoroughly rather than making general statements. Processing metrics help monitor analysis quality and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3dd5e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:57.564816Z",
     "iopub.status.busy": "2024-11-27T17:21:57.564425Z",
     "iopub.status.idle": "2024-11-27T17:21:57.592313Z",
     "shell.execute_reply": "2024-11-27T17:21:57.591017Z"
    },
    "papermill": {
     "duration": 0.036322,
     "end_time": "2024-11-27T17:21:57.594441",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.558119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResearchAnalyzer:\n",
    "    def __init__(self, model_name: str = 'gemini-1.5-flash-latest', \n",
    "                 config: Optional[GeminiConfig] = None):\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.config = config or GeminiConfig()\n",
    "        self.safety_settings = {\n",
    "            \"HARASSMENT\": \"block_none\",\n",
    "            \"HATE_SPEECH\": \"block_none\",\n",
    "            \"SEXUALLY_EXPLICIT\": \"block_none\",\n",
    "            \"DANGEROUS\": \"block_none\"\n",
    "        }\n",
    "        self.analysis_cache = []\n",
    "        self.tracker = TokenTracker()\n",
    "        self.validation_markers = {}\n",
    "        self.content_samples = {}\n",
    "        self.retry_threshold = 0.4  # Minimum acceptable coverage\n",
    "        self.max_retries = 2  # Maximum retries per batch\n",
    "\n",
    "    def stream_text(self, text: str) -> Iterator[str]:\n",
    "        words = text.split()\n",
    "        current_batch = []\n",
    "        word_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            current_batch.append(word)\n",
    "            word_count += 1\n",
    "            \n",
    "            if word_count >= self.config.batch_size:\n",
    "                yield ' '.join(current_batch)\n",
    "                current_batch = []\n",
    "                word_count = 0\n",
    "        \n",
    "        if current_batch:\n",
    "            yield ' '.join(current_batch)\n",
    "\n",
    "    def update_cache(self, analysis: str):\n",
    "        self.analysis_cache.append(analysis)\n",
    "        if len(self.analysis_cache) > self.config.cache_size:\n",
    "            self.analysis_cache.pop(0)\n",
    "\n",
    "    def get_cached_context(self) -> str:\n",
    "        if not self.analysis_cache:\n",
    "            return \"\"\n",
    "        \n",
    "        cache_text = \"\\n\\n---\\n\\n\".join(\n",
    "            f\"Analysis {i+1}:\\n{analysis}\" \n",
    "            for i, analysis in enumerate(self.analysis_cache[-3:])\n",
    "        )\n",
    "        \n",
    "        return f\"\\nPrevious analyses:\\n{cache_text}\" if cache_text else \"\"\n",
    "\n",
    "    def insert_validation_markers(self, text: str) -> str:\n",
    "        words = text.split()\n",
    "        marker_interval = self.config.batch_size // 4\n",
    "        marked_text = []\n",
    "        \n",
    "        for i in range(0, len(words), marker_interval):\n",
    "            context_start = max(0, i - 10)\n",
    "            context_end = min(len(words), i + 10)\n",
    "            context = ' '.join(words[context_start:context_end])\n",
    "            \n",
    "            marker_id = f\"MARKER_{i//marker_interval}\"\n",
    "            content_hash = hash(context) % 1000000\n",
    "            marker = f\"[{marker_id}_{content_hash}]\"\n",
    "            \n",
    "            self.content_samples[marker] = {\n",
    "                'context': context,\n",
    "                'position': i,\n",
    "                'referenced': False\n",
    "            }\n",
    "            \n",
    "            marked_text.extend(words[i:i+marker_interval])\n",
    "            marked_text.append(marker)\n",
    "        \n",
    "        return ' '.join(marked_text)\n",
    "\n",
    "    def validate_analysis(self, analysis: str) -> Dict[str, Any]:\n",
    "        validation_stats = {\n",
    "            'markers_found': 0,\n",
    "            'total_markers': len(self.content_samples),\n",
    "            'coverage_percentage': 0,\n",
    "            'content_matches': [],\n",
    "            'section_coverage': defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for marker, sample in self.content_samples.items():\n",
    "            content_words = set(sample['context'].lower().split())\n",
    "            analysis_words = set(analysis.lower().split())\n",
    "            overlap = len(content_words & analysis_words) / len(content_words)\n",
    "            \n",
    "            if overlap > 0.3:\n",
    "                validation_stats['markers_found'] += 1\n",
    "                validation_stats['content_matches'].append({\n",
    "                    'position': sample['position'],\n",
    "                    'overlap': overlap,\n",
    "                    'content': sample['context']\n",
    "                })\n",
    "                section = sample['position'] // self.config.batch_size\n",
    "                validation_stats['section_coverage'][section] += 1\n",
    "        \n",
    "        validation_stats['coverage_percentage'] = (\n",
    "            validation_stats['markers_found'] / validation_stats['total_markers'] * 100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nValidation Results:\")\n",
    "        print(f\"Coverage: {validation_stats['coverage_percentage']:.1f}%\")\n",
    "        print(f\"References found: {validation_stats['markers_found']}/{validation_stats['total_markers']}\")\n",
    "        \n",
    "        return validation_stats\n",
    "    def analyze_batch(self, batch: str, batch_count: int) -> str:\n",
    "        \"\"\"Analyze a single batch with improved error handling\"\"\"\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                self.build_prompt(batch, batch_count),\n",
    "                generation_config=self.config.get_generation_config(),\n",
    "                safety_settings=self.safety_settings\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_count}: {str(e)}\")\n",
    "            return \"\"  # Return empty string on error\n",
    "\n",
    "    def analyze(self, text: str) -> str:\n",
    "        marked_text = self.insert_validation_markers(text)\n",
    "        total_words = len(marked_text.split())\n",
    "        \n",
    "        self.tracker.start_tracking(total_words)\n",
    "        all_analyses = []\n",
    "        batch_validations = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch in self.stream_text(marked_text):\n",
    "            batch_count += 1\n",
    "            words = len(batch.split())\n",
    "            self.tracker.update(words)\n",
    "            \n",
    "            analysis = self.analyze_batch(batch, batch_count)\n",
    "            if analysis:  # Only process if we got a valid response\n",
    "                validation = self.validate_analysis(analysis)\n",
    "                batch_validations.append(validation)\n",
    "                self.update_cache(analysis)\n",
    "                all_analyses.append(analysis)\n",
    "        \n",
    "        self.tracker.close()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        total_coverage = sum(v['coverage_percentage'] for v in batch_validations) / len(batch_validations)\n",
    "        print(f\"\\nAnalysis Summary:\")\n",
    "        print(f\"Total batches: {batch_count}\")\n",
    "        print(f\"Average coverage: {total_coverage:.1f}%\")\n",
    "        \n",
    "        if batch_count > 1:\n",
    "            final_summary = self.generate_final_summary(all_analyses, batch_count)\n",
    "            final_validation = self.validate_analysis(final_summary)\n",
    "            print(f\"\\nFinal Summary Coverage: {final_validation['coverage_percentage']:.1f}%\")\n",
    "            return final_summary\n",
    "        \n",
    "        return all_analyses[0]\n",
    "        \n",
    "    def build_prompt(self, batch: str, batch_count: int, is_retry: bool = False) -> str:\n",
    "        \"\"\"Enhanced prompt with stronger emphasis on content coverage\"\"\"\n",
    "        base_prompt = f\"\"\"\n",
    "        Analyze this research content section {batch_count} thoroughly.\n",
    "        You MUST include specific quotes and references from the text to support your analysis.\n",
    "        \n",
    "        Requirements:\n",
    "        1. Quote at least 3 specific phrases or findings from different parts of the text\n",
    "        2. Refer to specific methodologies, techniques, or results mentioned\n",
    "        3. Identify unique or distinctive findings in this batch\n",
    "        4. Connect findings to previous batches where relevant\n",
    "        \"\"\"\n",
    "        \n",
    "        if is_retry:\n",
    "            base_prompt += \"\"\"\n",
    "            IMPORTANT: Your previous analysis didn't reference enough specific content.\n",
    "            Please ensure you:\n",
    "            - Quote MORE specific phrases from the text\n",
    "            - Reference MORE specific methods and findings\n",
    "            - Cover content from DIFFERENT PARTS of the text\n",
    "            - Be MORE explicit in connecting to the source material\n",
    "            \"\"\"\n",
    "            \n",
    "        base_prompt += f\"\"\"\n",
    "        Current text:\n",
    "        {batch}\n",
    "        \n",
    "        Previous context:\n",
    "        {self.get_cached_context()}\n",
    "        \"\"\"\n",
    "        \n",
    "        return base_prompt\n",
    "    \n",
    "    def stream_text_with_overlap(self, text: str, overlap_words: int) -> Iterator[str]:\n",
    "        \"\"\"Stream text in batches with overlap to maintain context\"\"\"\n",
    "        words = text.split()\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            end = min(start + self.config.batch_size, len(words))\n",
    "            if end < len(words):  # Not the last batch\n",
    "                # Find the nearest sentence end within the overlap window\n",
    "                window_end = end + overlap_words\n",
    "                window_text = ' '.join(words[end:min(window_end, len(words))])\n",
    "                sentence_end = self.find_sentence_end(window_text)\n",
    "                if sentence_end > 0:\n",
    "                    end += sentence_end\n",
    "                    \n",
    "            batch = ' '.join(words[start:end])\n",
    "            yield batch\n",
    "            start = end - overlap_words if end < len(words) else end\n",
    "            \n",
    "    @staticmethod\n",
    "    def find_sentence_end(text: str) -> int:\n",
    "        \"\"\"Find the nearest sentence end in text\"\"\"\n",
    "        sentence_ends = ['.', '!', '?']\n",
    "        min_pos = len(text)\n",
    "        \n",
    "        for end in sentence_ends:\n",
    "            pos = text.find(end)\n",
    "            if pos > 0 and pos < min_pos:\n",
    "                min_pos = pos + 1\n",
    "                \n",
    "        return min_pos if min_pos < len(text) else 0\n",
    "\n",
    "    def generate_final_summary(self, analyses: List[str], batch_count: int) -> str:\n",
    "        synthesis_prompt = f\"\"\"\n",
    "        Synthesize these {batch_count} analyses into a comprehensive summary:\n",
    "        {' '.join(analyses)}\n",
    "        \n",
    "        Provide:\n",
    "            1. Major research trends and breakthroughs\n",
    "            2. Cross-disciplinary patterns and influences\n",
    "            3. Technical and methodological innovations\n",
    "            4. Future research directions and implications\n",
    "            5. Key challenges and proposed solutions\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model.generate_content(\n",
    "            synthesis_prompt,\n",
    "            generation_config=self.config.get_generation_config(),\n",
    "            safety_settings=self.safety_settings\n",
    "        ).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02194bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:21:57.605385Z",
     "iopub.status.busy": "2024-11-27T17:21:57.605056Z",
     "iopub.status.idle": "2024-11-27T17:23:53.801106Z",
     "shell.execute_reply": "2024-11-27T17:23:53.799874Z"
    },
    "papermill": {
     "duration": 116.204184,
     "end_time": "2024-11-27T17:23:53.803346",
     "exception": false,
     "start_time": "2024-11-27T17:21:57.599162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building category mapping from 2,601,564 papers...\n",
      "Processed 0 papers\n",
      "Processed 100,000 papers\n",
      "Processed 200,000 papers\n",
      "Processed 300,000 papers\n",
      "Processed 400,000 papers\n",
      "Processed 500,000 papers\n",
      "Processed 600,000 papers\n",
      "Processed 700,000 papers\n",
      "Processed 800,000 papers\n",
      "Processed 900,000 papers\n",
      "Processed 1,000,000 papers\n",
      "Processed 1,100,000 papers\n",
      "Processed 1,200,000 papers\n",
      "Processed 1,300,000 papers\n",
      "Processed 1,400,000 papers\n",
      "Processed 1,500,000 papers\n",
      "Processed 1,600,000 papers\n",
      "Processed 1,700,000 papers\n",
      "Processed 1,800,000 papers\n",
      "Processed 1,900,000 papers\n",
      "Processed 2,000,000 papers\n",
      "Processed 2,100,000 papers\n",
      "Processed 2,200,000 papers\n",
      "Processed 2,300,000 papers\n",
      "Processed 2,400,000 papers\n",
      "Processed 2,500,000 papers\n",
      "Processed 2,600,000 papers\n",
      "\n",
      "Top 10 categories by paper count:\n",
      "cs.LG: 195,285 papers\n",
      "hep-ph: 183,180 papers\n",
      "hep-th: 169,522 papers\n",
      "quant-ph: 154,306 papers\n",
      "cs.CV: 137,773 papers\n",
      "gr-qc: 109,978 papers\n",
      "cs.AI: 106,352 papers\n",
      "astro-ph: 105,380 papers\n",
      "cond-mat.mtrl-sci: 95,198 papers\n",
      "cond-mat.mes-hall: 92,364 papers\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzers\n",
    "arxiv_file = '/kaggle/input/arxivjson/arxiv-metadata-oai-snapshot.json'\n",
    "arxiv_analyzer = ArxivAnalyzer(arxiv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d82379",
   "metadata": {
    "papermill": {
     "duration": 0.005896,
     "end_time": "2024-11-27T17:23:53.815437",
     "exception": false,
     "start_time": "2024-11-27T17:23:53.809541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The analysis of the ArXiv dataset, reveals machine learning and physics domains dominate research publications. The top 10 categories by paper count are:\n",
    "\n",
    "- Machine Learning (cs.LG): 195,285\n",
    "- High Energy Physics - Phenomenology (hep-ph): 183,180\n",
    "- High Energy Physics - Theory (hep-th): 169,522\n",
    "- Quantum Physics (quant-ph): 154,306\n",
    "- Computer Vision (cs.CV): 137,773\n",
    "- General Relativity & Quantum Cosmology (gr-qc): 109,978\n",
    "- Artificial Intelligence (cs.AI): 106,352\n",
    "- Astrophysics (astro-ph): 105,380\n",
    "- Materials Science (cond-mat.mtrl-sci): 95,198\n",
    "- Mesoscale & Nanoscale Physics (cond-mat.mes-hall): 92,364\n",
    "\n",
    "Given the significant representation of AI-related categories (cs.LG, cs.CV, cs.AI) and their relevance to current technological advances, we'll focus our analysis on these domains along with Computational Linguistics (cs.CL) and Statistical Machine Learning (stat.ML) to understand recent developments in artificial intelligence research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c52d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:23:53.829622Z",
     "iopub.status.busy": "2024-11-27T17:23:53.828499Z",
     "iopub.status.idle": "2024-11-27T17:25:14.849078Z",
     "shell.execute_reply": "2024-11-27T17:25:14.847807Z"
    },
    "papermill": {
     "duration": 81.030348,
     "end_time": "2024-11-27T17:25:14.851741",
     "exception": false,
     "start_time": "2024-11-27T17:23:53.821393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading papers from 2,601,564 entries...\n",
      "Processed 0 papers\n",
      "Processed 100,000 papers\n",
      "Processed 200,000 papers\n",
      "Processed 300,000 papers\n",
      "Processed 400,000 papers\n",
      "Processed 500,000 papers\n",
      "Processed 600,000 papers\n",
      "Processed 700,000 papers\n",
      "Processed 800,000 papers\n",
      "Processed 900,000 papers\n",
      "Processed 1,000,000 papers\n",
      "Processed 1,100,000 papers\n",
      "Processed 1,200,000 papers\n",
      "Processed 1,300,000 papers\n",
      "Processed 1,400,000 papers\n",
      "Processed 1,500,000 papers\n",
      "Processed 1,600,000 papers\n",
      "Processed 1,700,000 papers\n",
      "Processed 1,800,000 papers\n",
      "Processed 1,900,000 papers\n",
      "Processed 2,000,000 papers\n",
      "Processed 2,100,000 papers\n",
      "Processed 2,200,000 papers\n",
      "Processed 2,300,000 papers\n",
      "Processed 2,400,000 papers\n",
      "Processed 2,500,000 papers\n",
      "Processed 2,600,000 papers\n",
      "\n",
      "Paper counts by year:\n",
      "year\n",
      "2018    2500\n",
      "2019    2500\n",
      "2020    2500\n",
      "2021    2500\n",
      "2022    2500\n",
      "2023    2500\n",
      "2024    2500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "selected_categories = ['cs.AI', 'cs.CV', 'cs.LG', 'cs.CL', 'stat.ML']\n",
    "df = arxiv_analyzer.load_papers_by_category(selected_categories)\n",
    "\n",
    "# print(\"\\nPapers loaded by year:\")\n",
    "# print(df['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c1224e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:25:14.869846Z",
     "iopub.status.busy": "2024-11-27T17:25:14.869416Z",
     "iopub.status.idle": "2024-11-27T17:30:44.355978Z",
     "shell.execute_reply": "2024-11-27T17:30:44.354798Z"
    },
    "papermill": {
     "duration": 329.498277,
     "end_time": "2024-11-27T17:30:44.357959",
     "exception": false,
     "start_time": "2024-11-27T17:25:14.859682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 2,966,130 words\n",
      "Processed batch 1: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 56.7%\n",
      "References found: 34/60\n",
      "Processed batch 2: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 55.0%\n",
      "References found: 33/60\n",
      "Processed batch 3: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 83.3%\n",
      "References found: 50/60\n",
      "Processed batch 4: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 91.7%\n",
      "References found: 55/60\n",
      "Processed batch 5: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 85.0%\n",
      "References found: 51/60\n",
      "Processed batch 6: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 76.7%\n",
      "References found: 46/60\n",
      "Processed batch 7: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 78.3%\n",
      "References found: 47/60\n",
      "Processed batch 8: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 76.7%\n",
      "References found: 46/60\n",
      "Processed batch 9: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 73.3%\n",
      "References found: 44/60\n",
      "Processed batch 10: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 65.0%\n",
      "References found: 39/60\n",
      "Processed batch 11: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 65.0%\n",
      "References found: 39/60\n",
      "Processed batch 12: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 48.3%\n",
      "References found: 29/60\n",
      "Processed batch 13: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 63.3%\n",
      "References found: 38/60\n",
      "Processed batch 14: 200,000 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 51.7%\n",
      "References found: 31/60\n",
      "Processed batch 15: 166,130 words\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 43.3%\n",
      "References found: 26/60\n",
      "\n",
      "Processing Summary:\n",
      "Total words processed: 2,966,130\n",
      "Total tokens processed: 3,855,969\n",
      "Batches processed: 15\n",
      "Context window usage: 192.8%\n",
      "Processing time: 5.3 minutes\n",
      "Processing speed: 12188.7 tokens/second\n",
      "\n",
      "Analysis Summary:\n",
      "Total batches: 15\n",
      "Average coverage: 67.6%\n",
      "\n",
      "Validation Results:\n",
      "Coverage: 51.7%\n",
      "References found: 31/60\n",
      "\n",
      "Final Summary Coverage: 51.7%\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Comprehensive Summary of 15 Research Analyses on Machine Learning\n",
       "\n",
       "This document synthesizes 15 distinct research analyses, revealing major trends, innovations, and challenges in the field of machine learning.\n",
       "\n",
       "**1. Major Research Trends and Breakthroughs:**\n",
       "\n",
       "* **Efficient Optimization and Model Compression:**  Several studies focus on improving the efficiency of machine learning algorithms.  This includes developing novel optimization techniques like stochastic PDHG with high-probability convergence analysis (Analysis 1),  sparsifying input-hidden weights in ELMs (Analysis 1), and dramatically reducing the size of recurrent neural networks (Analysis 3) – achieving a 1KB model for wake-word recognition.  These breakthroughs address the computational cost and memory limitations associated with large models.\n",
       "\n",
       "* **Robustness and Generalization:**  A significant trend involves enhancing the robustness and generalization capabilities of models. This is evident in the development of robust representation learning techniques using InfoMax Autoencoders (Analysis 4),  handling significant rare events in reinforcement learning with the κ-operator (Analysis 4), and creating robust Q-functions for temporal difference learning (Analysis 4).  Improved generalization is also achieved in mapless driving (Analysis 7) and in handling long user histories in recommendation systems (Analysis 13).\n",
       "\n",
       "* **Addressing Data Limitations:**  Many studies address challenges related to limited or biased data.  This includes using synthetic data for fingerprint recognition (Analysis 9),  self-supervised clustering for plant disease classification (Analysis 7), weakly supervised water extraction (Analysis 9), few-shot learning for COVID-19 detection from ultrasound (Analysis 11), and developing contrastive learning methods for low-resource languages (Analysis 12).\n",
       "\n",
       "* **Explainability and Interpretability:**  The desire for more interpretable and explainable AI is reflected in several studies.  This includes developing interpretable manifold learning techniques (Analysis 6),  using explainable AI approaches for spatiotemporal visitation flows prediction (Analysis 13), and focusing on the interpretability of decisions in mapless driving (Analysis 7).\n",
       "\n",
       "* **Advancements in Specific Domains:** Significant progress is observed in specific application domains like autonomous driving (Analysis 7, Analysis 13), medical image analysis (Analysis 11, Analysis 12),  reinforcement learning (Analysis 1, Analysis 2, Analysis 4, Analysis 7, Analysis 12),  natural language processing (Analysis 10, Analysis 11, Analysis 12, Analysis 14), and graph neural networks (Analysis 13, Analysis 14).\n",
       "\n",
       "\n",
       "**2. Cross-Disciplinary Patterns and Influences:**\n",
       "\n",
       "* **Physics-Inspired Computing:** The use of memcomputing machines for RBM training (Analysis 0) demonstrates the influence of physics on machine learning algorithm design.\n",
       "\n",
       "* **Urban Planning and Transportation:**  The application of graph neural networks to predict spatiotemporal visitation flows (Analysis 13) and the development of reinforcement learning algorithms for urban air mobility fleet scheduling (Analysis 13) highlight the intersection of machine learning and urban planning.\n",
       "\n",
       "* **Medical Applications:**  Several studies focus on medical image analysis (Analysis 0, Analysis 11, Analysis 12), demonstrating the increasing importance of machine learning in healthcare.\n",
       "\n",
       "* **Social Sciences and Geopolitics:**  The lexicon-based sentiment analysis of the Ukrainian-Russian conflict (Analysis 12) illustrates the application of NLP techniques to social science research and geopolitical events.\n",
       "\n",
       "\n",
       "**3. Technical and Methodological Innovations:**\n",
       "\n",
       "* **Novel Optimization Algorithms:** Stochastic PDHG, κ-operator, accelerated inference for DPCNs.\n",
       "* **Novel Architectures:** FastRNN, FastGRNN, ELM-LC, CNAVR, IMAE, NFANet, CFC-Net, Cheetah.\n",
       "* **Novel Exploration Strategies:** MIME in reinforcement learning.\n",
       "* **Novel Sampling Schemes:**  For orthogonal matrices in Bayesian models.\n",
       "* **Novel Data Augmentation Techniques:**  Synthetic data generation for fingerprint recognition.\n",
       "* **Novel Evaluation Benchmarks:**  For text anonymization.\n",
       "* **Novel Frameworks:**  Unifying offline causal inference and online learning.\n",
       "\n",
       "\n",
       "**4. Future Research Directions and Implications:**\n",
       "\n",
       "* **Explainable and Trustworthy AI:**  Further research is needed to develop more interpretable and trustworthy machine learning models, addressing concerns about bias and fairness.\n",
       "\n",
       "* **Handling Long-Range Dependencies:**  Improving the ability of LLMs and other models to handle long-range dependencies in sequential data remains a crucial challenge.\n",
       "\n",
       "* **Efficient Training and Inference:**  Developing more efficient training and inference methods for large models is essential for scaling up AI applications.\n",
       "\n",
       "* **Addressing Data Scarcity:**  Developing more effective techniques for handling limited and biased data, especially in low-resource settings, is vital for broader AI accessibility.\n",
       "\n",
       "* **Generalization and Robustness:**  Further research is needed to improve the generalization and robustness of models across various domains and environments.\n",
       "\n",
       "* **Ethical Considerations:**  Research on the ethical implications of AI systems, particularly in workforce management (Analysis 3) and autonomous driving (Analysis 7, Analysis 13), needs to be prioritized.\n",
       "\n",
       "\n",
       "**5. Key Challenges and Proposed Solutions:**\n",
       "\n",
       "* **Hallucinations in LLMs:**  Several studies address the problem of hallucinations in LLMs, proposing various mitigation techniques.\n",
       "\n",
       "* **Data Scarcity:**  Many studies tackle data scarcity using synthetic data, self-supervised learning, few-shot learning, and contrastive learning.\n",
       "\n",
       "* **Computational Cost:**  Efficient optimization algorithms, model compression techniques, and accelerated inference methods are proposed to reduce computational costs.\n",
       "\n",
       "* **Generalization to Unseen Data:**  Robustness techniques and novel architectures are proposed to improve generalization to unseen data.\n",
       "\n",
       "* **Interpretability and Explainability:**  Methods for improving the interpretability and explainability of machine learning models are explored.\n",
       "\n",
       "* **Bias and Fairness:**  Addressing bias and ensuring fairness in AI systems is a recurring theme, requiring further research.\n",
       "\n",
       "\n",
       "In conclusion, these 15 analyses highlight a dynamic and rapidly evolving field.  Future research should focus on addressing the identified challenges, building upon the presented breakthroughs, and fostering cross-disciplinary collaborations to unlock the full potential of machine learning while mitigating potential risks.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Perform content analysis if we have papers\n",
    "if len(df) > 0:\n",
    "    content = ' '.join(df['abstract'].tolist())\n",
    "    config = GeminiConfig(batch_size=200000)\n",
    "    analyzer = ResearchAnalyzer(config=config)\n",
    "    analysis = analyzer.analyze(content)\n",
    "    display(Markdown(analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b33fe",
   "metadata": {
    "papermill": {
     "duration": 0.008327,
     "end_time": "2024-11-27T17:30:44.375275",
     "exception": false,
     "start_time": "2024-11-27T17:30:44.366948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6085780,
     "sourceId": 9905809,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 533.033245,
   "end_time": "2024-11-27T17:30:45.106429",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-27T17:21:52.073184",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
